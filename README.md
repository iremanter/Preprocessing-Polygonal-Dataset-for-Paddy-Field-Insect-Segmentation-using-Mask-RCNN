# Preprocessing-Polygonal-Dataset-for-Paddy-Field-Insect-Segmentation-using-Mask-RCNN

Paddy Pests Dataset has been labeled by using CVAT. During the annotation process polygonal annotation technique has been used. Polygonal annotation is a method of labeling objects in images by manually drawing a polygon around the object’s boundaries rather than using simple bounding boxes.

Class distribution, image resolution, and the presence of tiny objects have been examined. Here’s the detailed explanation of these factors and Paddy Pests Dataset” examination by considering these factors below:
● Class Distribution: This refers to the number of images per category such as different insect types in the dataset. A highly imbalanced dataset can lead to biased predictions. In “Paddy Pests Dataset”, class distribution is not good enough since there are not enough pest images. Therefore, applying data augmentation techniques was essential.
● Image resolution: Image resolution refers to the width × height of images in pixels. In “Paddy Pests Dataset”, resolution is pretty good and it’s easy to differentiate the insects and the backgrounds.
● The presence of tiny objects: In “Paddy Pests Dataset”, there are many small insects which makes detection difficult. Therefore, data augmentation techniques such as scaling help to overcome this difficulty.

Mask R-CNN:

The Mask R-CNN model for tiny object detection and classification has been implemented. Then, the model has been trained on the insect dataset. Moreover, proper handling of hyperparameters, such as learning rate, batch size, and optimizer selection, has been ensured. In the end, the model has been evaluated by calculating mean Average Precision (mAP). Also, challenges specific to detecting tiny objects, such as scale variance and occlusions have been addressed.

Previously, dataset preprocessing has been performed. As a first step, resizing and normalization were applied by standardizing image dimensions and pixel values. Therefore, uniformity of the input has been ensured. Secondly, all 135 images of insects in paddy fields were annotated by using CVAT. After the annotation process precise segmentation masks have been generated. After that, the data was split into train and test datasets for both model training and evaluation. The last is data augmentation before modelling and its techniques have been explained in detail above.

When it comes to model development, the 3.7 version of Python and Tensorflow framework are required for Mask-RCNN implementation. The required version (3.7) of Python was downloaded and the Tensorflow framework was implemented.
The stages of the Mask R-CNN model are Backbone Network, Region Proposal Network, RoIAlign, Classification and Bounding Box Regression Head, Mask Head. In the training code, r['rois'] refers to the Region of Interest (ROI) which is detected by the model. ROIs are bounding boxes around the objects, which the model has detected in the image. Each ROI is a rectangular box that marks the region where an object (insects) might be. These bounding boxes are generated by the Region Proposal Network (RPN). RPN is responsible for identifying areas of interest in the image that could contain objects. ROI Align and Mask Head generates segmentation masks for precise insect localization.

Uploading Images and Labeled Data:
As a first step 135 insect images were uploaded into the train folder and 10 randomly selected images uploaded into the validation folder. Also, labeled data uploaded into labels folder. Then, before diving into training code, all necessary libraries were imported.

Predicting:
The predicting code was created for running inference using a pre-trained Mask R-CNN model. It loaded the trained model with previously saved weights and applied it to a single image to detect and segment insects. The script has processed the image, run it through the network. After that, it has retrieved detection results including bounding boxes, masks, class labels, and confidence scores. Then, it visualized the results using mrcnn.visualize.display_instances(), overlaying detections on the input image. This code is useful for testing model performance on new images and validating the effectiveness of the trained model in real-world scenarios.

Fine-tuning:
Partial fine-tuning has been performed, where only the "heads" of the Mask R-CNN model are trained while the backbone is kept frozen.
The model is initialized with pretrained weights from the COCO dataset, but certain layers are excluded from loading the pretrained weights and are randomly initialized. By setting layers='heads', the training focuses on the newly added layers and leaving the rest of the network unchanged.
In this model, the mAP value was 0.64 before the fine tuning. After the fine-tuning approach was performed, the mAP value became 0.77. When mAP value is between 0.5 and 0.7, it is called as a good performance which means there are still improvement points, especially in some cases where the model misclassified or missed the insects. Fine-tuning has increased the mAP level and when the mAP level is above 0.7, it is called excellent performance where the model is detecting and segmenting insects in many images accurately.
